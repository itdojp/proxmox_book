# 第5章　ストレージ構成（ZFS / LVM / Ceph の基礎）

## 章のゴール

本章では、Proxmox VE で利用される主要なストレージ方式（LVM、ZFS、Ceph）の役割と特徴を整理し、
読者が自分のラボや小〜中規模環境に対して、どの方式を選ぶべきか判断できるようになることを目標とします。
本章の画面・操作例は Proxmox VE 9.1（9.x 系）を前提とします。

## この章で分かること / 分からないこと

- 分かること:
  - LVM / ZFS / Ceph のざっくりした違いと、選び分けの考え方
  - 単一ノードと小規模クラスタでの「現実的な」使い分け
- 分からないこと（後続章または別パスで扱います）:
  - 具体的な構築コマンドやチューニング（環境差が大きいため）
  - Ceph の詳細設計（ネットワーク/故障ドメイン/性能設計など）

## 用語メモ（最小）

- ストレージ（Proxmox の用語）: VM ディスク、ISO、バックアップ等を置く「保管場所」の定義（ローカル/共有/外部など）
- バックエンド: 実体として使う仕組み（LVM、ZFS、Ceph など）

## Proxmox VE におけるストレージの考え方

Proxmox VE では、「ストレージ」は仮想マシンやコンテナのディスクイメージ、ISO イメージ、バックアップなどを保管する論理的な単位として扱われます。
ローカルディスクを利用するストレージ、ネットワーク越しのストレージ、分散ストレージなど、複数の方式を組み合わせることができます。

本書のラボでは、次のような観点でストレージ方式を選びます。

- 単一ノードで完結するか、複数ノードで共有したいか
- 性能と冗長性のバランスをどこまで求めるか
- 運用の複雑さやトラブルシュートの難易度

## デフォルトで作られるストレージ（例: `local` / `local-lvm`）

初学者がつまずきやすい点として、「ISO はどこに置くのか」「VM のディスクはどこに作られるのか」があります。
Proxmox VE では、インストール直後から “用途の違うストレージ” が複数定義されていることが多いです。

代表例（典型的なインストールの場合）:

- `local`: ISO イメージやバックアップ、テンプレートなどを置くためのストレージ（ディレクトリ型であることが多い）
- `local-lvm`: VM ディスク（仮想ディスク）を置くためのストレージ（LVM-thin など）

補足:

- インストール時に ZFS を選ぶなど、ディスク構成によってストレージ名や構成は変わります。
- ストレージごとに「置けるもの（ISO/バックアップ/ディスクなど）」が決まっています。ISO のアップロード先に迷った場合は、そのストレージが ISO を扱える設定（コンテンツ種別）になっているかを確認してください。

Datacenter -> Storage 一覧の例:

![Datacenter -> Storage（例）](../../images/part2/ch5/01-datacenter-storage-list.png)

### 最小手順（Web UI: Datacenter → Storage）

1. 左のツリーで `Datacenter` をクリックする
2. 左のナビで `Storage` を開く
3. 一覧で、ストレージ名（例: `local` / `local-lvm`）と `Content`（何を置けるか）を確認する

Node -> Disks -> LVM-Thin の例（`local-lvm` の実体を把握する入口）:

![Node -> Disks -> LVM-Thin（例）](../../images/part2/ch5/02-node-local-lvm-lvmthin.png)

### 最小手順（Web UI: Node → Disks → LVM-Thin）

1. 左のツリーで対象ノードをクリックする
2. 左のナビで `Disks` → `LVM-Thin` を開く
3. `local-lvm` がどの Volume Group / Thinpool に紐づいているか（または相当する構成）を確認する

### スクショ無しでの最小確認（CLI）

スクリーンショットが無い段階でも、次の CLI を使うと「今どのストレージが使える状態か」「どこに何があるか」を最低限確認できます。

```bash
pvesm status
pvesm list local --content iso
pvesm list local --content backup
```

出力例（抜粋）:

```text
$ pvesm status
Name      Type     Status  Total     Used    Available  %
local     dir      active  100.00G   5.00G   95.00G     5.00%
local-lvm lvmthin  active   80.00G  20.00G   60.00G    25.00%

$ pvesm list local --content iso
Volid                                     Format  Type  Size
local:iso/ubuntu-24.04.1-live-server-amd64.iso iso     iso   <SIZE>

$ pvesm list local --content backup
Volid                                              Format   Type    Size
local:backup/vzdump-qemu-100-<YYYY_MM_DD-HH_MM_SS>.vma.zst vma.zst  backup  <SIZE>
...
```

見るポイント（最低限）:

- `pvesm status`: 対象ストレージが `active` で、空き容量がある
- `pvesm list ...`: ISO やバックアップが「どのストレージにあるか」を把握できる

ストレージ方式ごとの確認（使っている場合のみ）:

```bash
zpool status
lvs
```

## LVM ベースのローカルストレージ

LVM（Logical Volume Manager）は、Linux 標準のボリューム管理機能です。
Proxmox VE では、ローカルディスク上に LVM を構成し、その上に仮想マシンのディスクを作成する構成を利用できます。

### LVM を選ぶ場面

- 単一ノードのラボや、小規模な検証環境でシンプルに始めたい場合
- 既に LVM ベースでディスクを管理している環境を活用したい場合

メリット:

- セットアップが比較的簡単で、Linux の基本に馴染みがあれば理解しやすい
- 追加ディスクの増設や拡張が行いやすい

注意点:

- ローカルストレージであるため、他ノードとの共有には工夫が必要
- スナップショットやシンプロビジョニングの機能は、ZFS と比較すると限定的

## ZFS ベースのローカルストレージ

ZFS は、コピーオンライト方式のファイルシステム兼ボリュームマネージャで、スナップショットやチェックサムによるデータ保護機能を備えています。
Proxmox VE では、インストール時に ZFS を選択してホストのシステムディスクから ZFS を利用したり、データ用ディスクに ZFS プールを作成して VM を配置することができます。

### ZFS を選ぶ場面

- スナップショットやロールバックを多用したいラボ環境
- データ保護や自己修復機能を重視したい場合

メリット:

- スナップショットやクローンが高速に行える
- データの整合性チェックや自己修復機能がある

注意点:

- メモリ使用量が比較的多く、ホストのメモリに十分な余裕が必要
- 設定やチューニング項目が多く、慣れないとトラブルシュートに時間が掛かる

## Ceph による分散ストレージの概要

Ceph は、分散オブジェクトストレージ／ブロックストレージを提供するソフトウェアで、Proxmox VE と組み合わせることで、
複数ノードから共有できる仮想マシン用ストレージを構成できます。

本書のラボでは、3 ノードクラスタを前提に、教育的な規模で Ceph を利用するパターンを取り上げます。

### Ceph を選ぶ場面

- 複数ノード間で仮想マシンのディスクを共有し、ライブマイグレーションや HA を活用したい場合
- 将来的にノード数や容量を段階的に増やしたい場合

メリット:

- ノード間でストレージを共有でき、クラスタ構成と相性が良い
- 複数ディスク・複数ノードにデータを分散し、障害時の冗長性を確保できる

注意点:

- 小規模ラボでは、リソース要件（CPU・メモリ・ネットワーク）が負担になる場合がある
- 設計・運用が複雑であり、本番導入前には十分な検証が必要

## LVM / ZFS / Ceph の比較（概要）

| バックエンド | 代表的な用途 | 主なメリット | 注意点 |
| --- | --- | --- | --- |
| LVM | 単一ノードのシンプルなラボ構成 | セットアップが容易で慣れ親しんだ仕組みを活用できる | クラスタ構成での共有には工夫が必要、スナップショット機能は限定的 |
| ZFS | スナップショットを多用するラボ、データ保護を重視する環境 | 高速なスナップショット／クローン、整合性チェックと自己修復 | メモリを多く消費し、設定・チューニング項目が多い |
| Ceph | 3 ノード以上のクラスタで共有ストレージを提供したい環境 | ノード間での共有と冗長性を両立しやすい | リソース要件と設計の複雑さがあり、小規模ラボでは負荷になることがある |

## まず決めること（設計の早見表）

初心者は「バックエンドの詳細」よりも先に、次の 2 点を決めると迷いにくくなります。

1. VM ディスクを置く場所（高速/容量/冗長性の優先順位）
2. バックアップを置く場所（“別障害ドメイン” を意識できているか）

### 何をどこに置くか（最小）

| 置くもの | 置き場（例） | 理由 | 注意点 |
| --- | --- | --- | --- |
| ISO | `local`（Directory） | Web UI からアップロードしやすい | `local-lvm` には置けないことが多い（コンテンツ種別の前提） |
| VM ディスク | `local-lvm` / ZFS / Ceph | VM の I/O が集まりやすい | バックエンドごとにスナップショット/クローン/共有の前提が違う |
| バックアップ | 別ストレージ（推奨） | “戻せる” を担保する | ローカルだけに置くとノード障害で一緒に失う |

### 目的別のおすすめ（ラボ/小〜中規模）

| 目的 | 単一ノード | 3 ノードクラスタ | 一言 |
| --- | --- | --- | --- |
| まず動かす | LVM（`local-lvm`） | まずはローカルで開始 | 先に手を動かし、後から最適化する |
| スナップショットを多用 | ZFS | ZFS（各ノード） | 便利だがメモリと運用難度が上がる |
| 共有ストレージで HA/マイグレーション | （原則なし） | Ceph | 章7/8 とセットで考える（ネットワーク前提が重要） |

## ラボ構成ごとのストレージ選択パターン

Part 0 で紹介したラボパターンに対応させて、ストレージ構成の例を整理します。

### パターン A（単一ノードラボ）

- ホストローカルの LVM もしくは ZFS を利用し、シンプルな構成で VM を配置する
- バックアップは別ディスクや外部ストレージに退避し、必要に応じて復元する

### パターン B（3 ノードクラスタラボ）

- 共有ストレージとして教育目的の Ceph クラスタを構成し、VM ディスクを配置する
- 追加でローカル ZFS や LVM を組み合わせ、用途ごとにストレージを分ける

これらの関係は、`diagrams/part2/ch5/storage-layout.svg` の図として概略を示します。

## よくある設計・運用上の注意点

- すべてを「最強の構成」にしようとせず、ラボの目的に合ったシンプルさを優先する
- バックアップ先は、本番環境と同様に「別障害ドメイン」に置く意識を持つ
- 性能評価を行う場合は、ネスト構成やラボ特有の制約が結果に影響することを前提に読む

本章で整理した考え方をベースに、後続のストレージ・クラスタ・バックアップの章で具体的な設定手順や運用パターンを掘り下げていきます。

## まとめ

- Proxmox VE のストレージは LVM / ZFS / Ceph など複数のバックエンドがあり、目的（単一ノードかクラスタか、スナップショット重視か、など）で選び分けます。
- ラボでは「目的に合った最小構成」を優先し、いきなり複雑な構成（特に Ceph）にしないことが重要です。
- バックアップ先は別障害ドメインに置く意識を持ち、容量・運用も含めて設計します。
- 次に読む章: 第6章「ネットワーク設計と VLAN」で、通信経路の前提を整理します。
