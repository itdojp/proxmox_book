# 第7章　クラスタ構成と HA

## 章のゴール

本章では、Proxmox VE で 3 ノード程度のクラスタを構成し、
仮想マシンのライブマイグレーションや基本的な HA 動作を理解できるようになることを目標とします。
本章の画面・操作例は Proxmox VE 9.1（9.x 系）を前提とします。

## この章で分かること / 分からないこと

- 分かること:
  - クラスタで最低限押さえるべき概念（クォーラム、共有ストレージの前提など）
  - 「クラスタを作る → ノードを参加させる → HA を試す」という流れ
  - つまずきやすいポイント（多数決、ネットワーク分断、ストレージ前提）
- 分からないこと（後続章または別パスで扱います）:
  - UI を 1 クリックずつ追う作業手順（スクリーンショット取得後に追加）
  - 本番向けの高度なフェンシング/設計（環境依存が大きい）

## 事前準備（チェックリスト）

クラスタ作業は、前提が 1 つ崩れると復旧に時間がかかります。ラボでも次を先に確認しておくと安全です。

- 3 ノードのホスト名と固定 IP が確定している（例: `pve1/pve2/pve3`）
- ノード間通信ができる（同一セグメント、VLAN、MTU など）
- 時刻同期が取れている（NTP。証明書/クラスタで問題になりやすい）
- 共有ストレージの方針がある（例: Ceph、またはラボ用の代替手段）

## 用語メモ（最小）

- クォーラム: クラスタが「多数派」を満たしているかの判定
- corosync: ノード間通信とメンバーシップ/クォーラムに関わる仕組み
- 共有ストレージ: 複数ノードから同じ VM ディスクを参照できる仕組み（ライブマイグレーション/HA をスムーズにするための前提になりやすい）
- HA: ノード障害時に別ノードで VM を起動し直す、といった可用性の仕組み

## クラスタの基本概念

クラスタ構成では、複数ノードが協調して動作するために、次のような概念が重要になります。

- クォーラム
- ノードのメンバーシップ
- VM ディスクを別ノードから利用できる仕組み（例: 共有ストレージとしての Ceph、または ZFS を前提としたレプリケーション）

これらは、Part I のアーキテクチャ章や Part II のストレージ・ネットワーク章で触れたコンポーネントと密接に関係しています。

## ラボで想定する 3 ノードクラスタ

本書では、Part 0 で紹介したパターン B（3 ノードラボ）を前提とし、
3 台の Proxmox VE ノードを 1 つのクラスタとして構成する例を扱います。
クラスタ全体の構成イメージは、`diagrams/part3/ch7/cluster-ha.svg` を参照してください。

## クラスタ作成の流れ（概要）

1. 最初のノードでクラスタを作成する
2. 残りのノードをクラスタに参加させる
3. 必要に応じて、共有ストレージ（例: Ceph）を構成する

それぞれのステップでは、Web UI からクラスタ名やノード情報を入力し、
内部的には corosync などのコンポーネントが設定されます。
詳細な画面遷移やコマンドは、後続の具体的な手順セクションで扱う前提とし、ここでは流れと考え方に焦点を当てます。

### Web UI での入口（例）

クラスタ作成・参加の入口は、Web UI の **Datacenter → Cluster** です。

- 最初のノード: Datacenter → Cluster → Create Cluster からクラスタを作成する
- 追加するノード: Datacenter → Cluster → Join Cluster から参加させる

成功判定（最低限）:

- Datacenter → Cluster に、想定したノードが表示されている
- クォーラムが成立している（少なくとも “多数決が取れている” 状態になっている）

### CLI での成功判定（最小）

スクリーンショットが無い段階でも、次の CLI で「クラスタが成立しているか」を最低限確認できます。

- クォーラム確認: `pvecm status`
- ノード一覧: `pvecm nodes`

問題切り分けの入口（例）:

- `journalctl -u corosync -u pve-cluster --no-pager -n 50`

## HA 設定と基本的なテスト

クラスタが構成できたら、選択した仮想マシンに対して HA を有効化し、
ノード障害時に別ノードで自動起動されることを確認します。

テストの例:

- 対象 VM を HA グループに追加し、優先ノードを設定する
- 意図的にノードを停止し、別ノードで VM が起動するかを確認する

ラボ環境では、実際の障害試験を行う際に、ストレージやネットワークへの影響範囲を事前に把握しておくことが重要です。

### HA の状態確認（CLI）

HA を有効化した後に「今どのノードで動く想定か」「エラーになっていないか」を確認する入口として、次が使えます。

- HA 全体の状態: `ha-manager status`

## よくあるつまずきポイント

- クォーラムを満たさない構成（偶数ノード構成や、ノード停止時の多数決が取れないケース）
- 共有ストレージがない状態での期待しすぎ（ライブマイグレーションや HA には、ストレージの前提がある）
- ネットワーク分断時の挙動を理解していないことによる意図しない停止

これらのポイントは、Part II のネットワーク・ストレージ章と合わせて理解することで、
より安全なクラスタ設計ができるようになります。

## まとめ

- クラスタではクォーラムやノード間通信（corosync）など、単一ノードとは異なる前提があります。
- 「クラスタ作成 → ノード参加 →（必要なら）共有ストレージ → HA テスト」という流れで、少しずつ確認しながら進めます。
- つまずきやすいポイントはクォーラム不足、共有ストレージ前提の誤解、ネットワーク分断です。事前準備を確認してから作業します。
- 次に読む章: 第8章「バックアップ・リストアとレプリケーション」で、復元可能性を担保する運用を整理します。
