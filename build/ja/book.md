# 序章　本書の目的と読み方

## 本書の目的

本書は、Proxmox VE をこれから本格的に活用したいインフラエンジニアやシステムインテグレータに向けて、
実務で役に立つ知識と設計・運用の考え方を体系的に整理したものです。
単なる機能紹介ではなく、「なぜその設計にするのか」「どのような運用体制を前提にしているのか」をできるだけ言語化し、
読者が自分の現場に持ち帰って応用できることを目標とします。

また、Proxmox VE をまだ採用していない読者に対しても、
既存の仮想化基盤やクラウドサービスとの違い・組み合わせ方を理解するための材料を提供し、
新しい選択肢を検討する際の参考情報となることを目指します。

## 想定読者と前提知識

本書の主な読者像は次の通りです。

- これから Proxmox VE を評価・導入しようとしているインフラエンジニア
- 中小規模〜中堅規模のシステムインテグレータに所属し、オンプレミス環境の提案・構築を担当している技術者
- 既存の仮想化基盤（vSphere など）からの移行・並行運用を検討している技術リーダー

前提となる知識レベルは、概ね次のように想定しています。

- Linux サーバの基本的な操作（パッケージ管理、ログ確認、ネットワーク設定など）
- 仮想化の基本概念（ハイパーバイザ、仮想マシン、スナップショットなど）
- TCP/IP ネットワークの基礎（IP アドレス、ルーティング、VLAN の概要）

これらに不安がある場合でも、各章では重要な概念を簡単におさらいしながら説明します。
必要に応じて、外部ドキュメントや付録への参照も示します。

## 対象バージョンと表記

本書のスクリーンショットと UI 手順は、原則として **Proxmox VE 9.1（9.x 系）** を前提にしています。
Proxmox VE は定期的にアップデートされるため、マイナーバージョンの差で UI の位置や文言が変わることがあります。
その場合は、本書の「画面の開き方（どこをクリックするか）」を手がかりに読み替えてください。

また、本文中のホスト名・IP アドレス・ユーザー名などは説明のための例です。
読者の環境に合わせて置き換えて進めてください。

## 本書の構成

本書は大きく次のパートで構成されます。
詳細な章構成は `manuscript/ja/SUMMARY.md` にまとめられており、ここではその概要を示します。

- Part 0: 序章と執筆・検証環境の準備  
  本章（序章）で本書全体の目的と読み方を説明し、続く章でラボ環境の前提や検証用セットアップを整理します。
- Part I: Proxmox VE の概要と基本操作  
  Proxmox VE の位置づけとアーキテクチャを紹介し、インストール手順と仮想マシンの基本操作を一通り体験します。
- Part II: ストレージとネットワーク設計  
  ZFS や LVM、Ceph などのストレージ選択肢と、ブリッジ・ボンディング・VLAN を含むネットワーク設計を扱います。
- Part III: クラスタリング・HA・バックアップ  
  複数ノードによるクラスタ構成、高可用性 (HA)、バックアップとレプリケーションを組み合わせた保護戦略を解説します。
- Part IV: 運用・監視・エンタープライズ連携  
  日常運用や監視、トラブルシュートの考え方に加え、エンタープライズ環境との連携や設計パターンをまとめます。

各章の詳細なタイトルや順番は、今後の改訂や構成変更に応じて `SUMMARY.md` とともに見直される可能性があります。
その場合は、本章の記述も合わせて更新します。

## 学習の進め方

本書は、個人学習とチームでの学習のどちらにも利用できるように構成されています。

### 個人学習として読む場合

- Part 0 でラボ環境の前提を確認し、自分の PC や検証用サーバで再現できる範囲を決めます。
- Part I を通読し、まずは単一ノードの Proxmox VE をインストールして仮想マシンを操作できるようになることを目標とします。
- その後、自身の関心や業務に近いテーマ（ストレージ／ネットワーク／クラスタ／バックアップなど）から順に読み進めて構いません。
- 細かな設定値は暗記するのではなく、「なぜこの選択をしているのか」を意識しながら読み進めると、設計への応用がしやすくなります。

### チーム学習・社内勉強会で使う場合

- まず本章と Part 0・Part I をチームで共有し、本書が前提としているラボ環境や運用イメージを合わせます。
- 各メンバーに担当パートを割り振り、実際にラボ環境で手を動かしながら内容を確認します。
- 社内の標準や制約（利用できるストレージ、ネットワーク機器、監視基盤など）を書き出し、
  各章の内容を自社向けにどうアレンジするかを議論する材料として使ってください。

## Proxmox VE の位置づけ（簡単な概要）

Proxmox VE は、オープンソースの仮想化基盤として、KVM ベースの仮想マシンと LXC コンテナ、ストレージやネットワークの管理を一体的に提供するプラットフォームです。
Web ベースの管理 UI と API を備え、比較的少ない台数からクラスタ構成までスケールできるのが特徴です。

一般に、既存の商用ハイパーバイザ製品と比較すると、ライセンス体系や運用モデルが異なります。
本書では、特定ベンダー製品との詳しい比較は行いませんが、
「小規模〜中規模のオンプレミス環境で、どのような場面で Proxmox VE が有力な選択肢となり得るか」
という観点で、各章の解説の中に位置づけを織り込んでいきます。

以降の章では、ここで述べた目的と読み方を前提に、具体的なセットアップ手順や設計パターンを順番に掘り下げていきます。
# 執筆環境・検証環境の準備

本章では、本書の内容を実際に手を動かしながら追いかけるための環境づくりについて説明します。
読者が用意できるハードウェアや既存の仮想化基盤はさまざまですが、
ここでは「最低限この程度あれば本書の演習を一通り試せる」という目安と、
より踏み込んだ検証やクラスタ構成まで試したい場合の推奨構成を示します。

## 対象バージョン（Proxmox VE 9.1）

本書のスクリーンショットと UI 手順は、原則として **Proxmox VE 9.1（9.x 系）** を前提にしています。
Proxmox VE 9.1 は Debian Trixie (13.2) をベースとしており、標準のカーネルは 6.17 系です。

注意: Proxmox VE のバージョンアップでは、カーネル更新に伴ってドライバやカーネルモジュールの互換性問題が出ることがあります。
たとえば NVIDIA vGPU や LINSTOR/DRBD といった追加モジュールを利用する場合は、導入・更新前に公式の既知の問題を確認してください（例: `https://pve.proxmox.com/wiki/Roadmap` の “Known Issues & Breaking Changes”）。

## 最初に決めること（チェックリスト）

本書の手順は、ラボ環境の前提がぶれると途中で手戻りが発生します。
そこで、インストール作業に入る前に次の項目を決め、メモに残しておくことを推奨します。

- ノード構成: 単一ノードで進めるのか、3 ノードクラスタまで試すのか
- ホスト名（ノード名）: 例 `pve1` / `pve2` / `pve3`（章をまたいで参照するため、早めに固定する）
- 管理用 IP アドレス: 例 `192.168.10.11`（固定 IP を推奨）
- DNS/NTP: どこを参照するか（時刻ずれは証明書やクラスタ構成のトラブル原因になりやすい）
- ネットワーク分離: 管理用 / VM 用 / ストレージ用を分けるか（最初は 1 本でも開始可能）
- バックアップ退避先: 外付けディスク、NAS、別ホストなど（「別障害ドメイン」を意識する）

例（単一ノードラボ）:

- ノード: `pve1`
- 管理用ネットワーク: `192.168.10.0/24`（`pve1=192.168.10.11`）
- VM 用ネットワーク: 管理用と共用（最初は共用で開始し、必要なら VLAN で分離）

例（3 ノードクラスタラボ）:

- ノード: `pve1=192.168.10.11` / `pve2=192.168.10.12` / `pve3=192.168.10.13`
- 管理用ネットワーク（例 VLAN 10）と、VM/ストレージ用ネットワーク（例 VLAN 20/30）を分離（可能なら）

## 準備する PC / サーバのスペック

### 最低限のスペック（単一ノードで基本操作を試す）

まず、Proxmox VE を 1 台だけインストールし、仮想マシンの作成や基本操作を試すだけであれば、
次のようなスペックでも十分に学習を進められます。

- CPU: x86_64 対応 CPU（VT-x/AMD-V が有効であること）
- メモリ: 16 GB 以上（できれば 32 GB 以上）
- ストレージ: 500 GB 程度の SSD（SATA / NVMe は問わないが、SSD を推奨）
- ネットワーク: 1 GbE ポート 1 本以上

この構成であれば、いわゆる「自宅ラボ」向けの小型 PC や中古サーバでも十分に再現可能です。
複数の仮想マシンを同時に起動することを考えると、メモリは 16 GB よりも多いほど余裕が出ます。

### 推奨スペック（クラスタやストレージ検証を含めて試す）

クラスタ構成や Ceph などの分散ストレージも含めて検証したい場合は、
ある程度余裕のあるハードウェアを準備した方がスムーズです。
例えば、次のような構成が考えられます。

- CPU: 8 コア以上の x86_64 CPU
- メモリ: 64 GB 程度
- ストレージ:
  - Proxmox VE ノード用のシステムディスク（SSD）
  - VM 用データストアとしての SSD / HDD（複数台あると検証しやすい）
- ネットワーク:
  - 1 GbE ポート 2 本以上（管理用とストレージ／VM トラフィックを分けるイメージ）

このようなリソースを 1 台に集約し、後述する「ネスト構成のラボ」として利用することで、
物理サーバが 1 台しかない環境でも 3 ノードクラスタの挙動をある程度再現できます。

## 仮想化環境の選択肢

Proxmox VE を試すための「下側」の仮想化環境には、いくつかの選択肢があります。
本書では、次のようなパターンを代表例として扱います。

### 1. 物理サーバに直接インストールする（ベアメタル）

最もシンプルなのは、専用の物理サーバを用意して Proxmox VE を直接インストールする方法です。

- 利点:
  - 本番環境に近い構成で動作を確認できる
  - オーバーヘッドが少なく、性能評価もしやすい
- 注意点:
  - 物理サーバを占有する必要がある
  - 他の用途との兼用が難しい（既存のハイパーバイザと併用する場合は設計が必要）

本書では、スクリーンショットや画面遷移はベアメタルインストールを前提に説明しますが、
後述のネスト構成でもほぼ同じ手順で再現できます。

### 2. 既存のハイパーバイザ上にネスト構成で構築する

既に別の仮想化基盤（例: vSphere、Hyper-V、パブリッククラウド上のホスト型ハイパーバイザ）がある場合は、
その上に Proxmox VE の仮想マシンを 2〜3 台作成し、ネストされた環境として検証することもできます。

- 利点:
  - 追加の物理サーバを用意せずにクラスタや HA の挙動を試せる
  - スナップショットやテンプレートを活用して、検証環境を簡単に巻き戻せる
- 注意点:
  - ネスト構成の性能は本番と異なるため、性能評価の結果をそのまま本番に当てはめない
  - 上位ハイパーバイザ側でハードウェア支援仮想化を有効にする必要がある

本書で想定する 3 ノードクラスタのラボは、ネスト構成を前提として設計しています。

### 3. クラウド環境を利用する（必要に応じて）

一部のクラウド環境では、ベアメタルインスタンスやネスト構成を許容するインスタンス種別を利用して、
Proxmox VE を検証することも可能です。
ただし、クラウド環境でのライセンスや利用規約は必ず確認し、自己責任で利用してください。

## 本書で想定するラボ構成

本書では、次の 2 つのラボパターンを想定します。

### パターン A: 単一ノードラボ（基本編）

目的:
- インストール手順や基本的な VM 操作を一通り体験する。

構成イメージ:

- Proxmox VE ノード 1 台
- 管理用ネットワーク 1 本
- その上で複数の Linux/Windows VM を起動

このパターンは、Part I までの内容を追うのに十分です。
ストレージやネットワークもシンプルな構成で進められるため、最初の一歩として適しています。

### パターン B: 3 ノードクラスタ（発展編）

目的:
- クラスタ構成や HA、バックアップ／レプリケーションなど、Part II〜III の内容を実際に試す。

構成イメージ:

- Proxmox VE ノード 3 台（物理またはネスト構成）
- 共有ストレージとしての Ceph クラスタ、または共有ストレージに相当するストレージバックエンド
- 管理用ネットワークと、VM/ストレージ用ネットワーク（可能であれば分離）

ハードウェアが 1 台しかない場合でも、十分な CPU / メモリ / ストレージがあれば、
1 台の物理サーバの上に 3 台の Proxmox VE 仮想マシンを立てる形でこのパターンを再現できます。
後続のクラスタ／HA 章では、この 3 ノード構成を前提とした例を用います。

上記ラボ構成の概略は、`diagrams/part0/lab-topology.svg` に図としてまとめます。
本文中ではこの図を参照しながら、各ノードやネットワークの役割を説明していきます。

## 注意点（ストレージ・バックアップ）

Proxmox VE の検証環境といえども、ストレージとバックアップにはいくつか注意すべき点があります。

- システムディスクとデータディスクをできるだけ分ける  
  検証中に VM を大量に作成・削除すると、I/O が集中します。
  Proxmox VE 自体が動作しているシステムディスクとは別に、VM データ用のディスクを用意するとトラブルシュートがしやすくなります。
- SSD を優先的に利用する  
  特にネスト構成で複数ノードを同時に動かす場合、ストレージの速度がボトルネックになりがちです。
  可能な範囲で SSD を利用し、I/O の挙動を本番環境に近づけることを推奨します。
- バックアップ用の保存先をあらかじめ決めておく  
  外付けストレージや NAS、別ホスト上のバックアップサーバなど、バックアップデータを退避する場所を事前に決めておくと、
  Part III のバックアップ・レプリケーションの章にスムーズにつなげられます。
- 重要なデータを置かない  
  本書に沿って構築するラボ環境は、あくまで検証・学習用です。
  重要な業務データや本番環境のデータを保存することは避け、常に「いつでも作り直せる」前提で運用してください。

---

ここまでで、本書を進めるためのハードウェア要件と代表的なラボパターンを整理しました。
次の章以降では、このラボ環境を前提として Proxmox VE のインストールや基本操作、クラスタ構成などを順に見ていきます。
# 第1章　Proxmox VE の概要とポジショニング

## 章のゴール

この章では、Proxmox VE が「何をするための製品か」「どのような規模や用途に向いているか」をつかみ、
本書の後続章（インストール、VM 作成、ストレージ、ネットワーク、クラスタ、バックアップ）を読み進めるための前提を整えます。

## この章で分かること / 分からないこと

- 分かること:
  - Proxmox VE の基本的な位置づけ（VM とコンテナ、運用のイメージ）
  - 想定しやすい利用シーンと、向き・不向き
- 分からないこと（後続章で扱います）:
  - 具体的なインストール手順や、Web UI の画面操作
  - ストレージやネットワークの詳細設計、クラスタの具体設定

## 用語メモ（本書での呼び方）

本書では、最初に次の用語をこの意味で使います。

- ノード: Proxmox VE が動作しているサーバ（物理サーバ、またはネスト構成では Proxmox VE 用に作る仮想マシン）
- ゲスト: ノード上で動かす仮想マシン（VM）やコンテナ
- クラスタ: 複数ノードをまとめて管理・運用する構成

## Proxmox VE の概要

Proxmox VE（Virtual Environment）は、KVM ベースの仮想マシンと LXC コンテナを統合的に管理する、オープンソースの仮想化プラットフォームです。
Web ベースの管理インターフェースと REST API を備え、単一ノードから複数ノードのクラスタ構成まで、同じ操作感で扱えることを特徴としています。

Proxmox VE は、ハイパーバイザ、ストレージ管理、ネットワーク設定、バックアップ・レプリケーションといった仮想基盤に必要な要素を
一つの製品としてまとめて提供することを重視しています。
そのため、小規模〜中規模の環境でも、過度に多くのミドルウェアを組み合わせることなく、仮想化基盤を構築できます。

全体像のイメージは、`diagrams/part1/ch1/overview.svg` の概略図も参考にしてください。

## 想定する利用シーン

Proxmox VE は、次のような利用シーンで特に有力な選択肢となり得ます。

- 中小規模の企業・組織におけるオンプレミス仮想化基盤
- 社内業務システムや開発・検証環境の統合
- 教育機関やトレーニング環境におけるラボ用インフラ
- 自宅ラボや技術検証用の環境

一方で、数百〜数千ノード規模の大規模クラスタや、厳密なマルチテナント隔離が必要なケースでは、
専用のクラウドプラットフォームや別種のソリューションと組み合わせて利用する設計が検討されます。
本書では、主に「数台〜十数台程度のノードで構成される、小〜中規模の環境」を念頭に置いて解説します。

## 特徴と利点

Proxmox VE の代表的な特徴・利点として、次のような点が挙げられます。

- オープンソースであること  
  ソースコードが公開されており、コミュニティベースで改善が続けられています。
  有償サブスクリプションによるサポートを利用しつつ、オープンソースとしての透明性も確保されています。

- Web UI と CLI / API の両立  
  ブラウザからの直感的な操作と、スクリプトや自動化に活用できる API・コマンドラインツールが用意されています。
  少人数のチームでも運用しやすいバランスを意識した設計です。

- 仮想マシンとコンテナの統合管理  
  KVM ベースのフル仮想化と、LXC コンテナ型の仮想化を同一の UI から管理できます。
  ワークロードに応じて柔軟に使い分けられる点が、開発・検証環境などで特に有用です。

- クラスタとライブマイグレーションのサポート  
  複数ノードをクラスタとして構成し、仮想マシンのライブマイグレーションや HA を利用できます。
  小規模構成でも高可用性を意識した設計が取りやすくなります。

## 制約・注意点

一方で、Proxmox VE を採用する際には、いくつかの制約や注意点も理解しておく必要があります。

- 非機能要件や周辺エコシステムの違い  
  既存の商用ハイパーバイザ製品と比較すると、運用ツールや周辺製品との統合の仕方が異なります。
  監視・バックアップ・運用プロセスなどをどのように組み立てるかを事前に検討しておくことが重要です。

- 組織内の知見・サポート体制  
  Proxmox VE に慣れたメンバーが少ない組織では、トラブルシュートや設計判断に時間が掛かる場合があります。
  必要に応じて有償サポートを利用する、パートナー企業と連携するなど、運用体制をあらかじめ決めておくと安心です。

- 学習・検証のコスト  
  新しい仮想化基盤を導入する際には、移行や検証に一定の時間とリソースが必要です。
  本書では、後続の章で段階的にラボ構成を発展させながら学習できるように構成しています。

## 本書における扱い

本書では、Proxmox VE を「中小規模のオンプレミス環境で、現実的なコストと運用負荷で構築できる仮想化基盤」として位置づけます。
クラスタ構成や高可用性、バックアップ・レプリケーションといった機能を活用しつつ、
過度に複雑な構成を避けながら、現場で再現しやすい設計パターンに重点を置きます。

また、特定の他製品との優劣比較ではなく、
「どのような要件を持つプロジェクトであれば Proxmox VE が候補になり得るか」
という視点で解説を進めます。
読者自身が、自分の環境・組織にとって Proxmox VE が適切かどうかを判断できるようになることが、本章のゴールです。
# 第2章　アーキテクチャと主要コンポーネント

## 章のゴール

この章では、Proxmox VE を「いくつかの層（レイヤー）と主要コンポーネントの組み合わせ」として捉えられるようになり、
後続章の設定手順やトラブルシュートで「いま何を触っているのか」を見失わないことを目標にします。

## この章で分かること / 分からないこと

- 分かること:
  - Proxmox VE を構成する主要コンポーネントと役割（概略）
  - どの機能がどのレイヤーに属するか（ストレージ/ネットワーク/クラスタ等）
- 分からないこと（後続章で扱います）:
  - 画面操作の具体的な手順（どの画面で何を入力するか）
  - 個々のサービスの詳細設定・チューニング・ログの読み解き

## 用語メモ（本章でよく出る言葉）

- ノード: Proxmox VE が動作しているサーバ
- クラスタ: 複数ノードをまとめて管理・運用する構成
- `/etc/pve`: クラスタで共有される設定情報を参照できる領域（仕組みの詳細は深掘りしない）
- クォーラム: クラスタが「多数派を満たしている」かどうかの判断に使う考え方

## 本章のねらい

本章では、Proxmox VE を構成する主要なコンポーネントと、その役割・関係性を整理します。
具体的な設定手順に入る前に、どのサービスが何を担当しているのかを理解しておくことで、
トラブルシュートや設計判断を行いやすくすることが目的です。

## 主なコンポーネントと役割

Proxmox VE の内部では、いくつかのサービスやコンポーネントが連携して動作しています。
代表的なものと役割を、概略として以下に示します。

- Proxmox VE API／Web インターフェース  
  Web UI と REST API を提供し、ユーザーからの操作を受け付けます。
  仮想マシン／コンテナの作成・設定変更、ノードやクラスタの管理など、多くの操作はここを経由します。

- QEMU/KVM（仮想マシン）  
  フル仮想化の仮想マシンを実行するハイパーバイザ部分です。
  CPU・メモリ・ディスク・ネットワークといったリソースを仮想マシンに割り当てて動作させます。

- LXC（コンテナ）  
  OS コンテナ型の仮想化を提供します。
  軽量なコンテナ環境を、仮想マシンと同じ管理 UI から扱うことができます。

- pve-cluster（クラスタ管理）  
  ノード間で設定情報を共有し、一貫したクラスタ構成を維持するためのコンポーネントです。
  ノードが複数台ある場合でも、同じ設定やリソース定義を参照できるようにします。

- corosync（クラスタ通信・メンバーシップ）  
  クラスタ内のノード間通信やメンバーシップ管理に利用されます。
  ノードの生死やクォーラムの状態を監視し、HA の土台となる情報を提供します。

- ストレージスタック（LVM、ZFS、Ceph など）  
  仮想マシンやコンテナのディスクをどこに、どのような形で保存するかを担います。
  ローカルストレージ、共有ストレージ、分散ストレージなど、複数の選択肢があり、
  どの方式を選ぶかは設計上の重要なポイントになります。

- ネットワーク（Linux ブリッジ、ボンド、VLAN）  
  仮想マシンやコンテナにネットワーク接続を提供します。
  Linux ブリッジをベースに、ボンディングや VLAN タグなどを組み合わせて構成します。

これらのコンポーネントの関係性は、`diagrams/part1/ch2/architecture.svg` の図としてまとめます。

## 全体アーキテクチャのイメージ

Proxmox VE の典型的な構成を、概念的に整理すると次のような層構造として捉えられます。

- ハードウェア層  
  物理サーバ、CPU／メモリ、ローカルディスク、ネットワークインターフェースなど。

- ホスト OS と仮想化基盤層  
  Proxmox VE が提供するホスト OS、KVM／LXC、クラスタ管理コンポーネント（pve-cluster、corosync など）。

- ストレージ・ネットワークサービス層  
  LVM や ZFS、Ceph などのストレージバックエンド、および Linux ブリッジ／ボンド／VLAN によるネットワーク構成。

- ゲスト層（VM／コンテナ）  
  実際の業務システムや検証環境の OS・アプリケーションが動作する階層。

本書では、具体的な設定例やトラブルシュートの説明を行う際に、
どの層のどのコンポーネントに注目しているのかを意識しながら解説していきます。

## クラスタと HA に関わる要素

クラスタ構成や高可用性 (HA) を理解するうえで重要な要素を、ここで簡単に整理しておきます。
詳細な設定手順やシナリオは、Part III のクラスタ・HA 章で扱います。

- クォーラム  
  クラスタ内で「正常に動いているノードの多数派」が存在するかどうかを判断する仕組みです。
  corosync を通じてノード間の状態が共有され、クォーラムが失われた場合には危険な操作が抑制されます。

- フェンシング／ノード障害時の扱い  
  障害が発生したノードに対して、どのようにリソースを切り離すか、どのように再起動させるかといった方針を指します。
  実環境では外部電源装置や管理インターフェースと連携することもありますが、
  本書のラボでは、主にシンプルな構成で概念と挙動を確認します。

- リソース（VM／コンテナ）の HA ポリシー  
  どのリソースを HA 対象とするか、どのノードを優先的に利用するかといった設定です。
  Proxmox VE の Web UI から、対象 VM／コンテナごとに HA の有効化やグループ設定を行います。

これらの要素は、後続のクラスタ・HA 章でラボシナリオとともに詳しく扱いますが、
ここでは「どのコンポーネントが土台になっているか」を大まかに把握しておけば十分です。

## ストレージ構成との関係

Proxmox VE のアーキテクチャを理解するうえで、ストレージの扱いも重要です。
本書では、ストレージの詳細は Part II のストレージ章で解説しますが、
ここではアーキテクチャとの関係を簡単に触れておきます。

- ローカルストレージ  
  各ノード内のディスクを、LVM や ZFS などで管理し、仮想マシンのディスクを格納する構成です。
  単一ノード構成ではシンプルですが、クラスタでの共有・移動には工夫が必要です。

- 共有ストレージ／分散ストレージ  
  Ceph などの分散ストレージを利用することで、複数ノードから同じストレージプールにアクセスし、
  ライブマイグレーションや HA を前提とした設計が行いやすくなります。

- ストレージとバックアップ  
  バックアップやレプリケーション機能は、ストレージ構成と密接に関係します。
  バックアップ対象の保存先、ネットワーク帯域、復旧時間の要件などを踏まえて設計する必要があります。

これらの話題は、後続のストレージ・バックアップの章で具体的な設定例とともに掘り下げます。

## 本章のまとめと後続章との関係

本章では、Proxmox VE を構成する主要コンポーネントと、その役割・関係性を概観しました。
以降の章では、ここで紹介したコンポーネントを前提として、
インストール手順、基本操作、ストレージ・ネットワーク設計、クラスタ・HA、バックアップといったトピックを順に扱っていきます。

詳細な設定やチューニングに入る際には、必要に応じて本章の図（`diagrams/part1/ch2/architecture.svg`）やコンポーネント一覧を参照し、
「今どのレイヤー・どのサービスを触っているのか」を意識しながら読み進めてください。
# 第3章　Proxmox VE のインストール

## 章のゴール

この章では、読者が Proxmox VE を 1 台インストールし、初期設定と Web GUI へのアクセスまでを完了できるようになることを目標とします。
インストールの流れと、よくあるつまずきポイントを俯瞰しつつ、後続章で扱うクラスタ構成や運用に備えた最低限の前提を整えます。

## この章で分かること / 分からないこと

- 分かること:
  - インストール作業の全体像（どこで何を決めるか）
  - 初回ログインまでの確認ポイント（成功判定）
  - つまずきやすい箇所の切り分け
- 分からないこと（後続章で扱います）:
  - VM の作成・ゲスト OS インストール（第4章）
  - ストレージ方式の詳細選定（第5章）
  - ネットワーク設計の詳細（第6章）

## 対象読者と前提

この章は、既に仮想化の基本概念を理解しており、Part 0 で紹介したラボ環境を用意できている読者を想定しています。
具体的には次のような前提を置きます。

- 本章の画面・操作例は Proxmox VE 9.1（9.x 系）を前提としていること
- Proxmox VE をインストールするための物理サーバまたはネスト用の仮想マシン（将来的に Proxmox VE ノードとして利用するもの）を 1 台以上用意できること
- インストール対象ノードに対してコンソール接続（ディスプレイ／キーボード、もしくはリモートコンソール）が可能であること
- 基本的な BIOS/UEFI 設定の変更手順を把握していること

## インストール前の準備

インストール作業に入る前に、次の項目を確認しておきます。

- CPU の仮想化支援機能（Intel VT-x / AMD-V）が有効になっているか
- ブートモード（BIOS / UEFI）とブート順の設定
- Proxmox VE のインストール ISO イメージの入手と、USB メモリなどへの書き込み
- ネットワーク接続と IP アドレス割り当て方針（固定 IP を推奨）

これらの準備が整っていると、インストーラの画面遷移をスムーズに進めることができます。

補足: Proxmox VE 9.1 では、カーネル更新に伴うインターフェース名の変更でネットワーク設定が壊れないよう、
ネットワークインターフェース名を固定（pin）する仕組みが追加されています。
複数 NIC 構成や本番想定の構成では、インストール時点での検討ポイントになります。

### インストール前に決めること（入力値の例）

インストーラでは、後から変えにくい値（ホスト名、IP アドレス、インストール先ディスクなど）を入力します。
迷う場合は、まず次のような「学習用の固定値」を決めてから進めると手戻りが減ります。

- ノード名（ホスト名）: 例 `pve1`
- 管理用 IP: 例 `192.168.10.11/24`
- デフォルトゲートウェイ: 例 `192.168.10.1`
- DNS: 例 `192.168.10.1`（家庭用ルータ等）または組織の DNS
- タイムゾーン: 例 `Asia/Tokyo`

注意: インストール先ディスクの選択は取り返しがつきません。学習用の専用ディスクを使い、重要なデータが入ったディスクは選ばないでください。

## インストーラの画面遷移と入力項目

Proxmox VE のインストールメディアから起動すると、グラフィカルなインストーラが立ち上がります。典型的な画面の流れは次のようになります。

1. ライセンス確認
2. インストール先ディスクの選択とファイルシステムの設定
3. ロケール・タイムゾーン・キーボードレイアウトの設定
4. 管理者パスワードとメールアドレスの入力
5. ホスト名と初期ネットワーク設定の入力

各画面では、画面下部に表示されるエラーメッセージや注意書きを確認しながら進めます。
特に、インストール先ディスクの選択とネットワーク設定は、後から変更する際の影響が大きいため慎重に選択してください。

インストール全体の流れは、`diagrams/part1/ch3/install-flow.svg` に概略図としてまとめます。
実際の画面とは細部が異なる場合がありますが、どのタイミングでどの情報を入力するのかを把握する助けになります。

## 初期 Web GUI アクセスと最低限の確認

インストール完了後、再起動すると Proxmox VE ノードが起動し、コンソールには Web GUI へのアクセス URL が表示されます。
同じネットワーク上のクライアント端末からブラウザを開き、その URL にアクセスしてログインできることを確認します。

補足:
- URL は `https://<IPアドレス>:8006/` の形式になることが一般的ですが、まずは **コンソールに表示された URL** をそのまま使ってください。
- 初回アクセスでは証明書警告が表示されることがあります（学習環境では一般的）。本番環境では証明書の扱いを運用方針として決める必要があります。

初回ログイン時には、次のような点を確認しておくとよいでしょう。

- ノード名や IP アドレスが想定どおりに設定されているか
- ストレージタブに、インストール時に選択したストレージが正しく表示されているか
- 時刻設定が大きくずれていないか（必要に応じて NTP 設定を行う）

ここまで確認できれば、このノードは「第4章の VM 作成」に進める状態になっています。

## よくあるつまずきポイント

インストール時に遭遇しやすい問題として、次のようなものがあります。

- 起動順の設定ミスにより、インストールメディアからブートできない
- UEFI / Legacy BIOS の設定とインストールメディアの作成方法が一致していない
- ネットワーク設定の誤りにより、インストール後に Web GUI にアクセスできない

これらの問題を避けるためには、事前にラボ環境の設計と前提条件を整理しておくことが重要です。
詳細なトラブルシュートやクラスタ構成に関わる注意点は、後続の章で改めて扱います。

アクセスできないときの最小切り分け（例）:

- 管理用 PC から、コンソールに表示された IP へ疎通できるか（同一セグメント、VLAN、配線）
- ブラウザで `https://<IP>:8006/` を開けるか（ポート番号の付け忘れに注意）
- ノードのコンソールで、IP アドレスが想定どおりか（入力ミスや DHCP になっていないか）
# 第4章　仮想マシンの作成と基本操作

## 章のゴール

この章では、Proxmox VE 上で仮想マシンを作成し、起動・停止・コンソール接続・スナップショット取得といった基本操作を一通り体験できるようになることを目標とします。
本章の画面・操作例は Proxmox VE 9.1（9.x 系）を前提とします。

## この章で分かること / 分からないこと

- 分かること:
  - VM 作成ウィザードで「どこを何のために設定するか」
  - VM の起動・停止・コンソール接続などの基本操作
  - スナップショット/テンプレートの使いどころ（概念と注意点）
- 分からないこと（後続章で扱います）:
  - ストレージ方式ごとの最適化（第5章）
  - ネットワーク/VLAN の詳細設計（第6章）
  - クラスタ/HA と組み合わせた運用（第7章以降）

## 想定するゲスト OS

本書では、例として一般的な Linux ディストリビューション（例: Debian 系や Ubuntu Server）をゲスト OS として利用します。
ISO イメージの取得方法やライセンスは各ディストリビューションの公式ドキュメントに従ってください。

## 事前準備（チェックリスト）

第3章のインストールが完了し、Web UI にログインできる状態を前提とします。加えて、次の項目を確認しておくとスムーズです。

- ISO イメージが用意できている（後で Proxmox のストレージに配置する）
- VM を置くストレージに空きがある（ディスク容量に余裕がある）
- ノードのネットワーク（ブリッジ）が構成済みである（典型的には `vmbr0`）

## 仮想マシン作成ウィザード

Web GUI から「仮想マシンの作成」ウィザードを起動し、次のような項目を順に設定します。

- 一般設定（VM ID、名前）
- OS タブ（ゲスト OS タイプ、インストールメディアとしての ISO イメージ）
- システム設定（BIOS タイプ、マシンタイプなど、基本的にはデフォルトで問題ない）
- ディスク設定（ディスクサイズ、バス種別、ストレージの選択）
- CPU 設定（ソケット数・コア数）
- メモリ設定（割り当てメモリ量）
- ネットワーク設定（ブリッジ、モデル）

流れの全体像は `diagrams/part1/ch4/vm-create-flow.svg` にまとめます。

### 設定値の例（学習用の最小構成）

迷ったときは、まず「学習用の最小構成」として次のような値から始めるとよいでしょう（必要に応じて調整します）。

| 項目 | 例 | 補足 |
| --- | --- | --- |
| 名前 | `vm-ubuntu01` | 役割が分かる名前にする |
| vCPU | 2 | まずは小さく |
| メモリ | 2–4 GB | GUI なしのサーバ用途なら 2 GB から |
| ディスク | 20–40 GB | 後から拡張できることが多い |
| ネットワーク | `vmbr0` | 最初は管理ネットワークと共用でよい |

各ステップの詳細な画面は、`images/part1/ch4/` 以下に配置するスクリーンショットを参照してください。
取得対象の一覧は Issue #2 を参照してください（ウィザード各タブ、VM 概要/コンソール、スナップショット等）。

## 基本的な起動・停止・コンソール操作

仮想マシンを作成したら、次の操作を試してみます。

- 起動 / シャットダウン / 再起動
- コンソールへの接続（Web ブラウザ経由のコンソールビューア）
- ISO からのブートと OS インストールの開始

これらの操作は、VM 一覧から対象の仮想マシンを選択し、上部メニューのボタンやコンテキストメニューから行います。

成功判定（最低限）:

- VM を起動できる（エラーで止まらない）
- コンソールに接続でき、ISO からのブート画面（または OS インストーラ）が表示される
- ゲスト OS のインストール後、ゲスト側でネットワーク疎通が取れる（ping など）

## スナップショットとテンプレートの基礎

ゲスト OS のインストールや初期設定が完了したら、その状態をスナップショットとして保存しておくと便利です。
また、ベースとなる仮想マシンをテンプレート化し、同様の構成の VM を複数作成することもできます。

- スナップショット: 現在のディスク・メモリ状態を保存し、必要に応じてロールバックできる
- テンプレート: ベースイメージから新しい VM をすばやく複数作成する際に利用する

本書では、詳細なテンプレート運用や自動化は後続の章で扱い、ここでは「どのような場面で使うのか」をイメージできる程度に留めます。

注意:

- スナップショットは「バックアップの代わり」ではありません。バックアップは第8章で扱います。
- ストレージの種類によってはスナップショットの扱い（可否/方式/容量影響）が異なります（第5章）。

## よくあるつまずきポイント

仮想マシンの作成や起動時に遭遇しやすい問題として、次のようなものがあります。

- 割り当てメモリやディスク容量が不足しており、ゲスト OS のインストールに失敗する
- ネットワークブリッジの設定ミスにより、ゲスト OS から外部ネットワークに接続できない
- ISO イメージの選択を忘れ、何もブートできない状態になる

これらの問題に気づいた場合は、VM の設定を見直すか、一度削除して再作成することをためらう必要はありません。
ラボ環境では、「作り直せる状態を保つ」ことが重要です。
# 第5章　ストレージ構成（ZFS / LVM / Ceph の基礎）

## 章のゴール

本章では、Proxmox VE で利用される主要なストレージ方式（LVM、ZFS、Ceph）の役割と特徴を整理し、
読者が自分のラボや小〜中規模環境に対して、どの方式を選ぶべきか判断できるようになることを目標とします。
本章の画面・操作例は Proxmox VE 9.1（9.x 系）を前提とします。

## この章で分かること / 分からないこと

- 分かること:
  - LVM / ZFS / Ceph のざっくりした違いと、選び分けの考え方
  - 単一ノードと小規模クラスタでの「現実的な」使い分け
- 分からないこと（後続章または別パスで扱います）:
  - 具体的な構築コマンドやチューニング（環境差が大きいため）
  - Ceph の詳細設計（ネットワーク/故障ドメイン/性能設計など）

## 用語メモ（最小）

- ストレージ（Proxmox の用語）: VM ディスク、ISO、バックアップ等を置く「保管場所」の定義（ローカル/共有/外部など）
- バックエンド: 実体として使う仕組み（LVM、ZFS、Ceph など）

## Proxmox VE におけるストレージの考え方

Proxmox VE では、「ストレージ」は仮想マシンやコンテナのディスクイメージ、ISO イメージ、バックアップなどを保管する論理的な単位として扱われます。
ローカルディスクを利用するストレージ、ネットワーク越しのストレージ、分散ストレージなど、複数の方式を組み合わせることができます。

本書のラボでは、次のような観点でストレージ方式を選びます。

- 単一ノードで完結するか、複数ノードで共有したいか
- 性能と冗長性のバランスをどこまで求めるか
- 運用の複雑さやトラブルシュートの難易度

## LVM ベースのローカルストレージ

LVM（Logical Volume Manager）は、Linux 標準のボリューム管理機能です。
Proxmox VE では、ローカルディスク上に LVM を構成し、その上に仮想マシンのディスクを作成する構成を利用できます。

### LVM を選ぶ場面

- 単一ノードのラボや、小規模な検証環境でシンプルに始めたい場合
- 既に LVM ベースでディスクを管理している環境を活用したい場合

メリット:

- セットアップが比較的簡単で、Linux の基本に馴染みがあれば理解しやすい
- 追加ディスクの増設や拡張が行いやすい

注意点:

- ローカルストレージであるため、他ノードとの共有には工夫が必要
- スナップショットやシンプロビジョニングの機能は、ZFS と比較すると限定的

## ZFS ベースのローカルストレージ

ZFS は、コピーオンライト方式のファイルシステム兼ボリュームマネージャで、スナップショットやチェックサムによるデータ保護機能を備えています。
Proxmox VE では、インストール時に ZFS を選択してホストのシステムディスクから ZFS を利用したり、データ用ディスクに ZFS プールを作成して VM を配置することができます。

### ZFS を選ぶ場面

- スナップショットやロールバックを多用したいラボ環境
- データ保護や自己修復機能を重視したい場合

メリット:

- スナップショットやクローンが高速に行える
- データの整合性チェックや自己修復機能がある

注意点:

- メモリ使用量が比較的多く、ホストのメモリに十分な余裕が必要
- 設定やチューニング項目が多く、慣れないとトラブルシュートに時間が掛かる

## Ceph による分散ストレージの概要

Ceph は、分散オブジェクトストレージ／ブロックストレージを提供するソフトウェアで、Proxmox VE と組み合わせることで、
複数ノードから共有できる仮想マシン用ストレージを構成できます。

本書のラボでは、3 ノードクラスタを前提に、教育的な規模で Ceph を利用するパターンを取り上げます。

### Ceph を選ぶ場面

- 複数ノード間で仮想マシンのディスクを共有し、ライブマイグレーションや HA を活用したい場合
- 将来的にノード数や容量を段階的に増やしたい場合

メリット:

- ノード間でストレージを共有でき、クラスタ構成と相性が良い
- 複数ディスク・複数ノードにデータを分散し、障害時の冗長性を確保できる

注意点:

- 小規模ラボでは、リソース要件（CPU・メモリ・ネットワーク）が負担になる場合がある
- 設計・運用が複雑であり、本番導入前には十分な検証が必要

## LVM / ZFS / Ceph の比較（概要）

| バックエンド | 代表的な用途 | 主なメリット | 注意点 |
| --- | --- | --- | --- |
| LVM | 単一ノードのシンプルなラボ構成 | セットアップが容易で慣れ親しんだ仕組みを活用できる | クラスタ構成での共有には工夫が必要、スナップショット機能は限定的 |
| ZFS | スナップショットを多用するラボ、データ保護を重視する環境 | 高速なスナップショット／クローン、整合性チェックと自己修復 | メモリを多く消費し、設定・チューニング項目が多い |
| Ceph | 3 ノード以上のクラスタで共有ストレージを提供したい環境 | ノード間での共有と冗長性を両立しやすい | リソース要件と設計の複雑さがあり、小規模ラボでは負荷になることがある |

## ラボ構成ごとのストレージ選択パターン

Part 0 で紹介したラボパターンに対応させて、ストレージ構成の例を整理します。

### パターン A（単一ノードラボ）

- ホストローカルの LVM もしくは ZFS を利用し、シンプルな構成で VM を配置する
- バックアップは別ディスクや外部ストレージに退避し、必要に応じて復元する

### パターン B（3 ノードクラスタラボ）

- 共有ストレージとして教育目的の Ceph クラスタを構成し、VM ディスクを配置する
- 追加でローカル ZFS や LVM を組み合わせ、用途ごとにストレージを分ける

これらの関係は、`diagrams/part2/ch5/storage-layout.svg` の図として概略を示します。

## よくある設計・運用上の注意点

- すべてを「最強の構成」にしようとせず、ラボの目的に合ったシンプルさを優先する
- バックアップ先は、本番環境と同様に「別障害ドメイン」に置く意識を持つ
- 性能評価を行う場合は、ネスト構成やラボ特有の制約が結果に影響することを前提に読む

本章で整理した考え方をベースに、後続のストレージ・クラスタ・バックアップの章で具体的な設定手順や運用パターンを掘り下げていきます。
# 第6章　ネットワーク設計と VLAN

## 章のゴール

本章では、Proxmox VE でよく利用されるネットワーク構成（Linux ブリッジ、ボンディング、VLAN）の考え方を整理し、
単一ノードおよび小規模クラスタのラボ環境で再現しやすいパターンを身につけることを目標とします。
本章の画面・操作例は Proxmox VE 9.1（9.x 系）を前提とします。

## この章で分かること / 分からないこと

- 分かること:
  - Linux ブリッジ/ボンディング/VLAN の役割と、ラボでの組み合わせ方
  - 単一ノード/3 ノードクラスタでの「迷いにくい」ネットワーク分け
- 分からないこと（後続章または別パスで扱います）:
  - UI の画面操作を 1 クリックずつ追う手順（スクリーンショット取得後に追加）
  - SDN/EVPN などの発展トピック（本書では優先度低）

## 最初に決めること（チェックリスト）

ネットワークは後から変更すると影響範囲が大きいので、ラボでも最初に方針を決めておくと手戻りが減ります。

- 管理ネットワーク（ノードの Web UI に入るための経路）をどれにするか
- VM 用ネットワークを分けるか（最初は共用で開始し、必要なら VLAN で分離）
- ストレージ/バックアップ用ネットワークを分けるか（Ceph を使うなら検討）
- VLAN を使う場合: VLAN ID と用途の対応（例: 10=管理、20=VM、30=ストレージ）

例（学習用のシンプルな割り当て）:

- VLAN 10（管理）: `192.168.10.0/24`
- VLAN 20（VM）: `192.168.20.0/24`
- VLAN 30（ストレージ/バックアップ）: `192.168.30.0/24`（必要な場合のみ）

## 用語メモ（最小）

- ブリッジ: VM/コンテナを物理 NIC に「つなぐ」ための仮想スイッチ
- ボンド: 複数 NIC を束ねて冗長化/帯域確保する仕組み
- VLAN: 1 本のリンクを論理的に分割する仕組み（スイッチ側と整合が必要）

## ラボ環境で想定するネットワークパターン

Part 0 で紹介したラボパターンに合わせて、シンプルなネットワーク構成を想定します。

### パターン A（単一ノードラボ）

- 1 本の物理 NIC を Linux ブリッジ（例: vmbr0）として利用し、その上に VM の仮想 NIC を接続する
- 必要に応じて、管理用ネットワークとゲスト用ネットワークを VLAN で分離する

### パターン B（3 ノードクラスタラボ）

- 各ノードで、管理用と VM／ストレージ用のネットワークを分ける前提でブリッジを構成する
- クラスタ通信や Ceph 用トラフィックを流すネットワークは、可能であれば物理的または VLAN で分離する

これらの関係は、`diagrams/part2/ch6/network-topology.svg` に概略図として示します。

## Linux ブリッジの基本

Proxmox VE では、Linux ブリッジを用いて仮想マシンやコンテナを物理ネットワークに接続します。
標準インストール直後は、物理 NIC（例: eno1）に対して vmbr0 が作成され、そのブリッジにホスト自身と仮想マシンが接続される構成が一般的です。

## ボンディングの概要

冗長性や帯域確保が必要な場合、複数の物理 NIC をボンドインターフェースとして束ね、その上にブリッジを構成することができます。
ラボ環境では、実際にリンク障害を再現してみることで、フェイルオーバの動作を確認できます。

## VLAN の基本と Proxmox VE での扱い

VLAN を利用すると、1 本の物理リンク上で論理的にネットワークを分離できます。
Proxmox VE では、ブリッジインターフェース上に VLAN タグ付きのサブインターフェースを作成し、
VM の仮想 NIC に VLAN ID を割り当てることで、複数の VLAN を使い分けることができます。

ラボ環境では、次のような使い分けが考えられます。

- VLAN 10: 管理用ネットワーク
- VLAN 20: ゲスト VM 用ネットワーク
- VLAN 30: ストレージ／バックアップ用ネットワーク（必要に応じて）

これらの設定は、Proxmox VE の Web UI またはテキスト形式の設定ファイルを通じて行います。
本書では、具体的な CLI コマンドや設定ファイルの例は後続の詳細セクションまたは付録で扱う前提とし、ここでは設計の考え方とパターンに焦点を当てます。

## 設計時の注意点

- 単一障害点を避ける（管理用ネットワークやストレージトラフィックが 1 本のリンクに集中しないようにする）
- VLAN 設定はスイッチ側とホスト側で整合性を取る
- ラボ環境では、複雑な構成を無理に再現するのではなく、目的に合った最小限のパターンで検証する

本章で整理したネットワークパターンは、後続のクラスタ・Ceph・バックアップの章で利用される前提となります。
# 第7章　クラスタ構成と HA

## 章のゴール

本章では、Proxmox VE で 3 ノード程度のクラスタを構成し、
仮想マシンのライブマイグレーションや基本的な HA 動作を理解できるようになることを目標とします。
本章の画面・操作例は Proxmox VE 9.1（9.x 系）を前提とします。

## この章で分かること / 分からないこと

- 分かること:
  - クラスタで最低限押さえるべき概念（クォーラム、共有ストレージの前提など）
  - 「クラスタを作る → ノードを参加させる → HA を試す」という流れ
  - つまずきやすいポイント（多数決、ネットワーク分断、ストレージ前提）
- 分からないこと（後続章または別パスで扱います）:
  - UI を 1 クリックずつ追う作業手順（スクリーンショット取得後に追加）
  - 本番向けの高度なフェンシング/設計（環境依存が大きい）

## 事前準備（チェックリスト）

クラスタ作業は、前提が 1 つ崩れると復旧に時間がかかります。ラボでも次を先に確認しておくと安全です。

- 3 ノードのホスト名と固定 IP が確定している（例: `pve1/pve2/pve3`）
- ノード間通信ができる（同一セグメント、VLAN、MTU など）
- 時刻同期が取れている（NTP。証明書/クラスタで問題になりやすい）
- 共有ストレージの方針がある（例: Ceph、またはラボ用の代替手段）

## 用語メモ（最小）

- クォーラム: クラスタが「多数派」を満たしているかの判定
- corosync: ノード間通信とメンバーシップ/クォーラムに関わる仕組み
- 共有ストレージ: 複数ノードから同じ VM ディスクを参照できる仕組み（ライブマイグレーション/HA の前提になりやすい）
- HA: ノード障害時に別ノードで VM を起動し直す、といった可用性の仕組み

## クラスタの基本概念

クラスタ構成では、複数ノードが協調して動作するために、次のような概念が重要になります。

- クォーラム
- ノードのメンバーシップ
- 共有ストレージまたは等価の仕組み（例: Ceph）

これらは、Part I のアーキテクチャ章や Part II のストレージ・ネットワーク章で触れたコンポーネントと密接に関係しています。

## ラボで想定する 3 ノードクラスタ

本書では、Part 0 で紹介したパターン B（3 ノードラボ）を前提とし、
3 台の Proxmox VE ノードを 1 つのクラスタとして構成する例を扱います。
クラスタ全体の構成イメージは、`diagrams/part3/ch7/cluster-ha.svg` を参照してください。

## クラスタ作成の流れ（概要）

1. 最初のノードでクラスタを作成する
2. 残りのノードをクラスタに参加させる
3. 必要に応じて、共有ストレージ（例: Ceph）を構成する

それぞれのステップでは、Web UI からクラスタ名やノード情報を入力し、
内部的には corosync などのコンポーネントが設定されます。
詳細な画面遷移やコマンドは、後続の具体的な手順セクションで扱う前提とし、ここでは流れと考え方に焦点を当てます。

## HA 設定と基本的なテスト

クラスタが構成できたら、選択した仮想マシンに対して HA を有効化し、
ノード障害時に別ノードで自動起動されることを確認します。

テストの例:

- 対象 VM を HA グループに追加し、優先ノードを設定する
- 意図的にノードを停止し、別ノードで VM が起動するかを確認する

ラボ環境では、実際の障害試験を行う際に、ストレージやネットワークへの影響範囲を事前に把握しておくことが重要です。

## よくあるつまずきポイント

- クォーラムを満たさない構成（偶数ノード構成や、ノード停止時の多数決が取れないケース）
- 共有ストレージがない状態での期待しすぎ（ライブマイグレーションや HA には、ストレージの前提がある）
- ネットワーク分断時の挙動を理解していないことによる意図しない停止

これらのポイントは、Part II のネットワーク・ストレージ章と合わせて理解することで、
より安全なクラスタ設計ができるようになります。
# 第8章　バックアップ・リストアとレプリケーション

## 章のゴール

本章では、Proxmox VE のバックアップ機能とレプリケーション機能の基本的な考え方を理解し、
ラボ環境でバックアップジョブの作成・実行・リストアを試せるようになることを目標とします。
本章の画面・操作例は Proxmox VE 9.1（9.x 系）を前提とします。

## この章で分かること / 分からないこと

- 分かること:
  - バックアップとリストアの基本（何を、どこに、どの頻度で）
  - 「復元できること」を確認するための、最小限のテストの考え方
  - クラスタ環境でのレプリケーションの位置づけ（概念）
- 分からないこと（後続章または別パスで扱います）:
  - UI を 1 クリックずつ追う手順（スクリーンショット取得後に追加）
  - Proxmox Backup Server（PBS）を含む本番向けの詳細設計（環境依存が大きい）

## 事前準備（チェックリスト）

バックアップは「保管場所」が無いと始まりません。ラボでも最初に次を確認しておくと手戻りが減ります。

- バックアップの保存先を決めている（ローカル以外を推奨: 外付け/NAS/別ホスト など）
- バックアップ対象の VM（テスト用）が 1 台ある（壊してもよい VM）
- 復元テストのやり方を決めている（同じノードか、別ノードか、VMID を変えるか）

## 用語メモ（最小）

- バックアップ: VM/コンテナを「あとで戻せる形」で保存すること
- リストア: バックアップから VM/コンテナを復元すること
- レプリケーション: ある VM の状態を別ノードへ定期的に複製し、障害時の起動を早くするための仕組み（クラスタ向け）

## バックアップの基本概念

Proxmox VE では、仮想マシンやコンテナの状態をバックアップとして取得し、
別のストレージに保存することで、障害や誤操作からの復旧に備えます。

バックアップ時に意識すべきポイント:

- どのストレージにバックアップを保存するか（ローカルディスク、外部ストレージなど）
- どのくらいの頻度でバックアップを取得するか
- どの単位（VM 単位、サービス単位）でバックアップを考えるか

重要: 「バックアップを取る」だけでは不十分です。**実際に復元できること** を定期的に確認してください。
本章では、ラボでできる最小限の復元テストを紹介します。

## バックアップジョブの作成イメージ

Web UI からバックアップジョブを作成し、対象となる VM / コンテナ、保存先ストレージ、スケジュールなどを指定します。
ラボ環境では、まずは手動実行や低頻度のスケジュールで動作を確認するところから始めると良いでしょう。

流れの全体像は `diagrams/part3/ch8/backup-restore-flow.svg` にまとめます。

### 例: ラボ用バックアップ方針（最小）

迷う場合は、まず次のような「学習用の最小方針」から始めるとよいでしょう。

- 対象: テスト用 VM 1 台（例: `vm-ubuntu01`）
- 保存先: 別ストレージ（例: 外付け/NAS/別ホスト。可能なら “別障害ドメイン”）
- 取得頻度: まずは手動実行で 1 回、次に 1 日 1 回など
- 世代管理（保持数）: 少数（例: 3 世代）から始める

スクリーンショット（TODO）:
- 取得対象（Issue #2）:
  - `images/part3/ch8/01-datacenter-backup-jobs.png` Datacenter -> Backup ジョブ一覧画面
  - `images/part3/ch8/02-create-backup-job-wizard.png` 新規バックアップジョブ作成ウィザード
  - `images/part3/ch8/03-manual-backup-task-log.png` 手動バックアップ実行時のタスクログ画面
  - `images/part3/ch8/04-restore-dialog.png` バックアップ一覧画面からのリストアダイアログ
  - `images/part3/ch8/05-replication-job-settings.png` レプリケーションジョブ設定画面（ノード間レプリケーションの例）

## リストアの考え方

取得したバックアップから VM をリストアする際には、次の点を意識します。

- 同じホストに戻すか、別ノードに復元するか
- 既存の VM と競合しないように ID やストレージを調整する

ラボ環境では、意図的にテスト用の VM をバックアップ・削除・リストアする一連の流れを試し、
どの程度の時間と手順が必要かを体感しておくことが重要です。

### 復元テスト（ラボでの最小チェック）

本書では次のような「壊してよい VM」で練習することを推奨します。

1. テスト用 VM のバックアップを 1 回取得する
2. 別 VMID（または別ノード）としてリストアする
3. リストアした VM が起動できることを確認する

成功判定（最低限）:

- リストア処理がエラーなく完了する
- リストアした VM が起動できる
- ゲスト OS にログインでき、最低限の疎通（ping 等）が取れる

## レプリケーションの概要

クラスタ環境では、特定の VM を定期的に別ノードへレプリケーションすることで、
障害時に素早く起動できる待機系を用意することができます。

レプリケーションのパターンの一例:

- ノード A 上の重要な VM を、ノード B に定期レプリケーションする
- HA 設定と組み合わせて、フェイルオーバ先での起動を想定する

注意:

- レプリケーションはストレージ構成や前提条件に依存します（「どのストレージで可能か」は要確認）。
- ラボでは「バックアップ/リストア」と混同しないよう、目的（RPO/RTO の違い）を意識してください。

## ラボでの実践パターン

- 単一ノードラボでは、まずは（別ディスクや外部ストレージなどの）バックアップ先へのバックアップとリストアを通じて基本の流れを確認する
- 3 ノードクラスタラボでは、共有ストレージやレプリケーションを利用した復旧シナリオを検証する

これらの練習を通じて、「バックアップを取っているつもり」ではなく、
実際に復元できるかどうかを確認する習慣を身につけることが、本章の狙いです。
# 第9章　運用・監視・トラブルシュート

## 章のゴール

本章では、Proxmox VE 環境を日常的に運用するうえでの基本的なタスクと、
監視・ログ確認・トラブルシュートの入り口となる考え方を整理します。

## この章で分かること / 分からないこと

- 分かること:
  - 日次/週次/月次でやると良い “最低限の運用”
  - まず何を見て、何を切り分けるか（トラブルシュートの入口）
- 分からないこと（別パスで扱います）:
  - 特定の監視製品（Prometheus など）に依存した詳細設計
  - カーネル/ハードウェア故障の深いデバッグ（環境差が大きい）

## 用語メモ（最小）

- タスク（Tasks）: Web UI で実行された操作の履歴（成功/失敗の入口になりやすい）
- Syslog: ノードのログの入口（詳細はホスト OS のログも併用する）
- Runbook: 手順を標準化した運用メモ（「誰がやっても同じになる」ことが目的）

## 最初に見る場所（チェックリスト）

トラブル時にいきなり深掘りすると迷子になりがちです。まずは次の順で “入口” を確認します。

1. 何が起きたか（症状・影響範囲・発生時刻）をメモする
2. Web UI の Task History / Syslog で、直近の失敗タスクやエラーを確認する
3. ノード/ゲストの概要画面（リソースグラフ）で、CPU/メモリ/ディスクI/O の異常がないか見る
4. 必要に応じてホスト OS のログ（`journalctl` など）へ進む

図としての全体像は `diagrams/part4/ch9/triage-flow.svg` を参照してください。

スクリーンショット（TODO）:
- 取得対象（Issue #2）:
  - `images/part4/ch9/01-node-syslog.png` Web UI の Syslog 画面（ノード単位）
  - `images/part4/ch9/02-task-history.png` Task History 画面（ジョブの成功／失敗が分かる例）
  - `images/part4/ch9/03-node-dashboard-resource-graphs.png` ノードダッシュボードのリソースグラフ（CPU / メモリ / ディスク I/O）

## 日次・週次・月次の運用チェックリスト（例）

### 日次

- バックアップジョブの結果確認
- 重要な VM / コンテナの稼働状況確認
- リソース使用率（CPU / メモリ / ストレージ）の簡易チェック

### 週次

- ストレージ使用量の推移確認と将来予測のメモ
- クラスタ状態（ノードの状態、クォーラム）の確認
- 監視システムのアラート履歴レビュー

### 月次

- バックアップ・リストア手順のリハーサル
- バージョンアップやパッチ適用の検討（リリースノートと既知の問題を確認し、必要ならメンテナンス計画を立てる）
- 運用ドキュメントや Runbook の見直し

## ログと基本的な指標の読み方（概要）

Proxmox VE の運用では、Web UI やホスト OS 上のログ、監視ツールを組み合わせて状態を把握します。

- Web UI の「Syslog」や「Task History」で、直近のジョブやエラーを確認する
- ホスト OS のログファイル（例: journalctl や /var/log/syslog）を参照し、ハードウェアやサービスの異常を確認する
- CPU / メモリ / ストレージ I/O のグラフを俯瞰し、平常と異なるパターンがないかを見る

## 代表的なトラブルシュートシナリオ（例）

### ケース 1: VM に接続できない

- ネットワーク経路の確認（VM の NIC 設定、ブリッジ、スイッチ側の設定など）
- 直近の設定変更やメンテナンスの有無を確認する

### ケース 2: ストレージの空き容量が逼迫している

- 不要なスナップショットやテンポラリディスクの整理
- バックアップ先の容量とローテーションポリシーの見直し
- 将来的な容量増設の検討

### ケース 3: バックアップジョブが失敗する

- エラーメッセージとログの確認
- ネットワークやストレージへの到達性確認
- 対象 VM の状態（ロック状態や負荷状況）の確認

これらのシナリオはあくまで入口であり、実際のトラブルシュートでは環境固有の要素も考慮する必要があります。
重要なのは、「何がいつからおかしくなったのか」「どのコンポーネントに影響がありそうか」を切り分ける視点です。
# 第10章　エンタープライズ連携・事例ベースの設計指針

## 章のゴール

本章では、Proxmox VE をエンタープライズ環境に組み込む際の代表的な連携ポイントと、
実案件を抽象化した設計パターンを整理します。

## この章で分かること / 分からないこと

- 分かること:
  - エンタープライズ環境で “必ず話題になる” 連携ポイント（ID、バックアップ、監視、ネットワーク/セキュリティ）
  - 実案件を抽象化した設計パターン（どういう順序で合意形成すると安全か）
- 分からないこと（別パスで扱います）:
  - 特定の製品（特定の監視/バックアップ製品など）に依存した詳細手順
  - 実在組織の具体的な構成・数値（機密のため扱わない）

## 初心者向けの読み方

この章は「すぐに手を動かすための手順」ではなく、**本番導入で詰まりやすい論点の地図**です。
初心者の読者は、まず次の 2 点を押さえれば十分です。

1. Proxmox VE 単体では完結せず、組織の既存標準（ID/監視/バックアップ等）と結びつく
2. 技術の前に「運用と合意」が必要な領域がある（権限設計、復元テスト、監視ルールなど）

全体像は `diagrams/part4/ch10/integration-map.svg` を参照してください。

## 用語メモ（最小）

- 認証/認可: 「ログインできるか」「何をしてよいか」の制御（権限設計が重要）
- 監視/ログ: 障害を “検知” して “原因を追う” ための仕組み（第9章と接続）
- バックアップ: 失われたものを “戻す” ための仕組み（第8章と接続）

## 典型的な連携ポイント

- ID / 認証基盤（例: ディレクトリサービスとの連携）
- バックアップ・アーカイブ基盤
- 監視・ログ収集基盤
- ネットワーク・セキュリティポリシー（ファイアウォール、セグメント設計など）

## シナリオ例（匿名化・抽象化されたケース）

### シナリオ 1: 既存ディレクトリサービスとの連携

- Proxmox VE の管理者認証をディレクトリサービスと連携させ、
  役割に応じたアクセス権限を付与するパターン

### シナリオ 2: 既存バックアップ基盤との統合

- Proxmox VE のバックアップデータを、既存のバックアップストレージやアーカイブシステムへ送るパターン

### シナリオ 3: 監視・インシデント対応プロセスへの組み込み

- Proxmox VE のメトリクスやイベントを既存の監視ツールに統合し、
  既存のオンコール体制やインシデント対応フローに組み込むパターン

## 設計指針（Do / Don’t の例）

- Do: 既存の標準（ID 管理、バックアップポリシー、監視ルール）に合わせて Proxmox VE を組み込む
- Do: ラボ環境で連携パターンを検証し、運用チームと合意形成してから本番導入する
- Don’t: Proxmox VE 用だけに独立した運用プロセスを乱立させる
- Don’t: 監視・バックアップ・セキュリティポリシーを「後で考える」として先送りにする

## まとめ

本章で紹介した連携ポイントやシナリオはあくまで一例ですが、
重要なのは、「Proxmox VE だけ」を見ずに、組織全体の標準や既存の仕組みとの整合性を意識することです。
本書で学んだ技術的な知識を、組織のルールや運用プロセスと結びつけていくことで、
より現実的で持続可能な仮想化基盤の設計が可能になります。
